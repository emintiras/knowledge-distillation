2025-03-16 12:29:38,519 - __main__ - INFO - Using device: cuda
2025-03-16 12:29:38,519 - src.data_loader - INFO - Initializing data loaders...
2025-03-16 12:29:38,519 - src.data_loader - INFO - Loading the CIFAR-10 dataset
2025-03-16 12:29:39,659 - src.data_loader - INFO - Data loaders initialized successfully with batch size 128.
2025-03-16 12:29:39,659 - src.data_loader - INFO - CIFAR-10 Train Size: 50000, Test Size: 10000
2025-03-16 12:29:39,659 - __main__ - INFO - Data loaders initialized.
2025-03-16 12:29:39,659 - __main__ - INFO - Loading pretrained teacher model...
2025-03-16 12:29:39,765 - __main__ - INFO - Pretrained teacher model loaded successfully.
2025-03-16 12:29:39,766 - __main__ - INFO - Training student model WITHOUT knowledge distillation...
2025-03-16 12:29:39,775 - src.train_student - INFO - Epoch [1/10] started.
2025-03-16 12:29:52,457 - src.train_student - INFO - Epoch 1, Training Loss: 8.137, Test Loss: 1.353, Accuracy: 51.01%
2025-03-16 12:29:52,457 - src.train_student - INFO - Epoch [2/10] started.
2025-03-16 12:30:04,253 - src.train_student - INFO - Epoch 2, Training Loss: 6.883, Test Loss: 1.234, Accuracy: 55.94%
2025-03-16 12:30:04,253 - src.train_student - INFO - Epoch [3/10] started.
2025-03-16 12:30:15,500 - src.train_student - INFO - Epoch 3, Training Loss: 6.344, Test Loss: 1.100, Accuracy: 59.99%
2025-03-16 12:30:15,500 - src.train_student - INFO - Epoch [4/10] started.
2025-03-16 12:30:26,959 - src.train_student - INFO - Epoch 4, Training Loss: 5.959, Test Loss: 1.109, Accuracy: 60.55%
2025-03-16 12:30:26,959 - src.train_student - INFO - Epoch [5/10] started.
2025-03-16 12:30:38,155 - src.train_student - INFO - Epoch 5, Training Loss: 5.775, Test Loss: 1.063, Accuracy: 62.26%
2025-03-16 12:30:38,155 - src.train_student - INFO - Epoch [6/10] started.
2025-03-16 12:30:49,386 - src.train_student - INFO - Epoch 6, Training Loss: 5.550, Test Loss: 1.029, Accuracy: 62.89%
2025-03-16 12:30:49,386 - src.train_student - INFO - Epoch [7/10] started.
2025-03-16 12:31:00,594 - src.train_student - INFO - Epoch 7, Training Loss: 5.438, Test Loss: 1.051, Accuracy: 63.37%
2025-03-16 12:31:00,594 - src.train_student - INFO - Epoch [8/10] started.
2025-03-16 12:31:11,782 - src.train_student - INFO - Epoch 8, Training Loss: 5.290, Test Loss: 0.968, Accuracy: 65.93%
2025-03-16 12:31:11,782 - src.train_student - INFO - Epoch [9/10] started.
2025-03-16 12:31:22,758 - src.train_student - INFO - Epoch 9, Training Loss: 5.166, Test Loss: 0.935, Accuracy: 67.22%
2025-03-16 12:31:22,759 - src.train_student - INFO - Epoch [10/10] started.
2025-03-16 12:31:33,873 - src.train_student - INFO - Epoch 10, Training Loss: 5.087, Test Loss: 0.959, Accuracy: 66.52%
2025-03-16 12:31:33,903 - __main__ - INFO - Student model WITHOUT knowledge distillation trained and saved.
2025-03-16 12:31:33,903 - __main__ - INFO - Training student model WITH knowledge distillation...
2025-03-16 12:31:33,905 - src.train_knowledge_distillation - INFO - Epoch [1/10] started.
2025-03-16 12:31:46,171 - src.train_knowledge_distillation - INFO - Epoch 1, Loss: 2.571
2025-03-16 12:31:46,171 - src.train_knowledge_distillation - INFO - Epoch [2/10] started.
2025-03-16 12:31:58,640 - src.train_knowledge_distillation - INFO - Epoch 2, Loss: 1.830
2025-03-16 12:31:58,640 - src.train_knowledge_distillation - INFO - Epoch [3/10] started.
2025-03-16 12:32:11,440 - src.train_knowledge_distillation - INFO - Epoch 3, Loss: 1.558
2025-03-16 12:32:11,440 - src.train_knowledge_distillation - INFO - Epoch [4/10] started.
2025-03-16 12:32:24,034 - src.train_knowledge_distillation - INFO - Epoch 4, Loss: 1.437
2025-03-16 12:32:24,034 - src.train_knowledge_distillation - INFO - Epoch [5/10] started.
2025-03-16 12:32:36,972 - src.train_knowledge_distillation - INFO - Epoch 5, Loss: 1.339
2025-03-16 12:32:36,973 - src.train_knowledge_distillation - INFO - Epoch [6/10] started.
2025-03-16 12:32:48,895 - src.train_knowledge_distillation - INFO - Epoch 6, Loss: 1.261
2025-03-16 12:32:48,895 - src.train_knowledge_distillation - INFO - Epoch [7/10] started.
2025-03-16 12:33:01,002 - src.train_knowledge_distillation - INFO - Epoch 7, Loss: 1.212
2025-03-16 12:33:01,003 - src.train_knowledge_distillation - INFO - Epoch [8/10] started.
2025-03-16 12:33:13,295 - src.train_knowledge_distillation - INFO - Epoch 8, Loss: 1.178
2025-03-16 12:33:13,296 - src.train_knowledge_distillation - INFO - Epoch [9/10] started.
2025-03-16 12:33:25,236 - src.train_knowledge_distillation - INFO - Epoch 9, Loss: 1.148
2025-03-16 12:33:25,236 - src.train_knowledge_distillation - INFO - Epoch [10/10] started.
2025-03-16 12:33:36,979 - src.train_knowledge_distillation - INFO - Epoch 10, Loss: 1.121
2025-03-16 12:33:36,998 - __main__ - INFO - Student model WITH knowledge distillation trained and saved.
2025-03-16 12:33:36,998 - __main__ - INFO - === EVALUATION RESULTS ===
2025-03-16 12:33:36,998 - __main__ - INFO - Testing student model WITHOUT knowledge distillation:
2025-03-16 12:33:38,263 - src.train_student - INFO - Accuracy: 66.52%
2025-03-16 12:33:38,263 - __main__ - INFO - Testing student model WITH knowledge distillation:
2025-03-16 12:33:39,529 - src.train_student - INFO - Accuracy: 70.05%
2025-03-16 12:33:39,529 - __main__ - INFO - Testing teacher model (for reference):
2025-03-16 12:33:41,119 - src.train_teacher - INFO - Accuracy: 79.44%
2025-03-16 12:33:41,119 - __main__ - INFO - 
=== COMPARATIVE RESULTS ===
2025-03-16 12:33:41,119 - __main__ - INFO - Teacher model accuracy: 79.44%
2025-03-16 12:33:41,119 - __main__ - INFO - Student WITHOUT KD accuracy: 66.52%
2025-03-16 12:33:41,119 - __main__ - INFO - Student WITH KD accuracy: 70.05%
2025-03-16 12:33:41,119 - __main__ - INFO - Improvement due to knowledge distillation: 3.53%
2025-03-16 12:33:41,119 - __main__ - INFO - Gap to teacher (WITH KD): 9.39%
2025-03-16 12:33:41,119 - __main__ - INFO - Gap to teacher (WITHOUT KD): 12.92%
